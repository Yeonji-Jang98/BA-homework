{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한글 문서의 분류\n",
    "다음무비(http://movie.daum.net)로부터 crawl한 영화리뷰를 이용하여 분류 연습<br>\n",
    "영화리뷰와 영화의 제목을 학습해서 주어진 리뷰내용으로 어떤 영화에 대한 리뷰인지를 예측하고자 함\n",
    "인스티즈포털(https://www.instiz.net/name_enter?category=0)에서 크롤링한 아이돌 검색 별 게시글 제목을 이용하여 분류연습<br>\n",
    "게시글 제목과 아이돌 분류를 학습해서 어떤 아이돌에 대한 게시물 제목인지 예측\n",
    "### data file 내용\n",
    "방탄소년단, 엑소, 트와이스, 아이즈원에 대한 게시글제목 데이터임\n",
    "총 2280개의 데이터\n",
    "<U+00...>등의 필요없는 문자열을 제거하려고 했으나, R과 파이썬을 전부 이용해도 지워지지 않고, 엑셀에서 수동적으로 수정해서 저장하면 계속 인코딩 문제가 발생했음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "text = []\n",
    "y = []\n",
    "with open('아이돌.csv', encoding='utf-8') as csvfile:\n",
    "    csvreader = csv.reader(csvfile)\n",
    "    for row in csvreader:\n",
    "        #print(row)\n",
    "        if row: #그 줄에 내용이 있는 경우에만\n",
    "            text.append(row[0]) #게시글 제목을 text 리스트에 추가\n",
    "            y.append(row[1]) #아이돌 그룹명을 text 리스트에 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of samples: 2280\n",
      "아이돌 게시글: {'BTS', 'exo', 'twice', 'izone'}\n"
     ]
    }
   ],
   "source": [
    "print('Num of samples: {}'.format(len(text)))\n",
    "print('아이돌 게시글: {}'.format(set(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data and labels into a training and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(text, y, random_state=0)\n",
    "# 비율을 지정하지 않으면 75:25로 분할됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1710"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt #konlpy에서 Twitter 형태소 분석기를 import\n",
    "#from konlpy.tag import Twitter #konlpy에서 Twitter 형태소 분석기를 import\n",
    "twitter_tag = Okt()\n",
    "#twitter_tag = Twitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<', 'U', '+', '0001', 'F', '496', '>', '월일', '에', '뜬', '쯔위', '컨셉', '사진', '움짤', '모음', '<', 'U', '+', '0001', 'F', '496', '>']\n"
     ]
    }
   ],
   "source": [
    "print(twitter_tag.morphs(X_train[1])) #둘째 게시글제목에 대해 형태소 단위로 tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['월일', '쯔위', '컨셉', '사진', '움짤', '모음']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_tag.nouns(X_train[1]) #둘째 게시글 제목에서 명사만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twit_tokenizer(text): # Twitter 형태소 분석기의 명사추출함수를 tokenizer 함수로 사용\n",
    "    return twitter_tag.nouns(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['혹쉬 최근에 입덕한 원스들 있니 ',\n",
       " '멜프라니ㅣ이ㅣㅠㅠㅠㅠㅠ',\n",
       " '아쪼 인스타 봤니   ',\n",
       " '나 개 다 보냈는데 지금 혹시 몰라 다시 누르니까 또 보내져',\n",
       " '유리 ost 오늘 공개래ㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠㅠ',\n",
       " '<U+0001F525><U+0001F525><U+0001F525><U+0001F525><U+0001F525>ㅊㅇㄷ 다들 하고 있지  탄소들<U+0001F618><U+0001F525><U+0001F525><U+0001F525><U+0001F525>',\n",
       " '눈들 얼마나 있어 ',\n",
       " '아아앜 ㅠㅠㅠㅠ 왜 매번 다른 사람들이 빠지냐구ㅠㅠㅠㅠ',\n",
       " '이번엔 티저 때문에 뮤비가 인기급상승동영상에 안 올라가는 일 없길 <U+0001F64F>',\n",
       " '공지<U+0001F4E2><U+0001F4E2><U+0001F4E2><U+0001F4E2>']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:10] #test data에서 앞 10개를 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['twice', 'twice', 'exo', 'BTS', 'izone', 'BTS', 'izone', 'BTS',\n",
       "       'twice', 'BTS'], dtype='<U5')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X_test_tfidf[:10]) # test data의 예측내용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['twice', 'twice', 'izone', 'BTS', 'izone', 'BTS', 'izone', 'BTS', 'twice', 'exo']\n"
     ]
    }
   ],
   "source": [
    "print(y_test[:10]) # test data 실제 아이돌 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 성능을 개선하기 위한 노력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<', 'U', '+', '0001', 'F', '496', '>', '월일', '에', '뜬', '쯔위', '컨셉', '사진', '움짤', '모음', '<', 'U', '+', '0001', 'F', '496', '>']\n"
     ]
    }
   ],
   "source": [
    "# morphs()는 명사 외에도 모든 형태소를 포함\n",
    "print(twitter_tag.morphs(X_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.8830409356725146\n",
      "Test score 0.6491228070175439\n",
      "(1710, 1243)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YeonJI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\YeonJI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=twitter_tag.morphs, min_df=2) # 명사 대신 모든 형태소를 사용\n",
    "#tfidf = TfidfVectorizer(tokenizer=twit_tokenizer, min_df=3, max_df=0.90, max_features=1000, use_idf=True, sublinear_tf=True)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "print('Train score', clf.score(X_train_tfidf, y_train))\n",
    "print('Test score', clf.score(X_test_tfidf, y_test))\n",
    "print(X_train_tfidf.shape)\n",
    "#명사만 사용한 것에 비해 train score는 상승, test score는 하락"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('<', 'Punctuation'), ('U', 'Alpha'), ('+', 'Punctuation'), ('0001', 'Number'), ('F', 'Alpha'), ('496', 'Number'), ('>', 'Punctuation'), ('월일', 'Noun'), ('에', 'Josa'), ('뜨다', 'Verb'), ('쯔위', 'Noun'), ('컨셉', 'Noun'), ('사진', 'Noun'), ('움짤', 'Noun'), ('모음', 'Noun'), ('<', 'Punctuation'), ('U', 'Alpha'), ('+', 'Punctuation'), ('0001', 'Number'), ('F', 'Alpha'), ('496', 'Number'), ('>', 'Punctuation')]\n"
     ]
    }
   ],
   "source": [
    "print(twitter_tag.pos(X_train[1], norm=True, stem=True)) #pos()는 형태소와 품사를 함께 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def twit_tokenizer2(text): #전체를 다 사용하는 대신, 명사, 동사, 형용사를 사용\n",
    "    target_tags = ['Noun', 'Verb', 'Adjective']\n",
    "    result = []\n",
    "    for word, tag in twitter_tag.pos(text, norm=True, stem=True):\n",
    "        if tag in target_tags:\n",
    "            result.append(word)\n",
    "#            result.append('/'.join([word, tag]))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['월일', '뜨다', '쯔위', '컨셉', '사진', '움짤', '모음']\n"
     ]
    }
   ],
   "source": [
    "print(twit_tokenizer2(X_train[1])) # 사용 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.8619883040935673\n",
      "Test score 0.6631578947368421\n",
      "(1710, 911)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=twit_tokenizer2, min_df=2) #명사, 동사, 형용사를 이용하여 tfidf 생성\n",
    "#tfidf = TfidfVectorizer(tokenizer=twit_tokenizer, min_df=3, max_df=0.90, max_features=1000, use_idf=True, sublinear_tf=True)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "print('Train score', clf.score(X_train_tfidf, y_train))\n",
    "print('Test score', clf.score(X_test_tfidf, y_test))\n",
    "print(X_train_tfidf.shape)\n",
    "# 현재까지 중에서 test score가 가장 뛰어남"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 형태소를 다 사용하고 품사를 알 수 있도록 하면?\n",
    "def twit_tokenizer3(text):\n",
    "    #target_tags = ['Noun', 'Verb', 'Adjective']\n",
    "    result = []\n",
    "    for word, tag in twitter_tag.pos(text, norm=True, stem=True):\n",
    "        result.append('/'.join([word, tag])) #단어의 품사를 구분할 수 있도록 함\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score 0.87953216374269\n",
      "Test score 0.6578947368421053\n",
      "(1710, 1197)\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=twit_tokenizer3, min_df=2)\n",
    "#tfidf = TfidfVectorizer(tokenizer=twit_tokenizer, min_df=3, max_df=0.90, max_features=1000, use_idf=True, sublinear_tf=True)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "print('Train score', clf.score(X_train_tfidf, y_train))\n",
    "print('Test score', clf.score(X_test_tfidf, y_test))\n",
    "print(X_train_tfidf.shape)\n",
    "#Train score 상승, #Test score 하락"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.944\n",
      "Test set score: 0.676\n"
     ]
    }
   ],
   "source": [
    "# train score가 높으므로 ridge를 쓰면 어떨까?\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "ridge_clf = RidgeClassifier(alpha = 1)\n",
    "ridge_clf.fit(X_train_tfidf, y_train)\n",
    "print('Train set score: {:.3f}'.format(ridge_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score: {:.3f}'.format(ridge_clf.score(X_test_tfidf, y_test)))\n",
    "# train score가 올라가는 현상이 벌어짐\n",
    "# test score도 함께 증가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.773\n",
      "Test set score: 0.649\n",
      "Used features count: 387 out of 1197\n"
     ]
    }
   ],
   "source": [
    "#lasso를 쓰면?\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "lasso_clf = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "lasso_clf.fit(X_train_tfidf, y_train)\n",
    "print('Train set score: {:.3f}'.format(lasso_clf.score(X_train_tfidf, y_train)))\n",
    "print('Test set score: {:.3f}'.format(lasso_clf.score(X_test_tfidf, y_test)))\n",
    "print('Used features count: {}'.format(np.sum(lasso_clf.coef_ != 0)), 'out of', X_train_tfidf.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0530101  0.00714228 0.00929344 0.00910356 0.00847454 0.00790744\n",
      " 0.0074683  0.00726786 0.00715722 0.00707493 0.0068332  0.00650216\n",
      " 0.00628227 0.00615433 0.0060278  0.00590501 0.00577303 0.00553344\n",
      " 0.00543666 0.00540344 0.00521521 0.00515999 0.00502858 0.00489193\n",
      " 0.00487598 0.00483086 0.00468443 0.0046638  0.0044988  0.00439197\n",
      " 0.00426873 0.00427881 0.00415167 0.00402148 0.00399458 0.00392669\n",
      " 0.0038507  0.00381235 0.00377764 0.0037254  0.00363985 0.00362168\n",
      " 0.00347248 0.00347083 0.00340898 0.00334986 0.00331703 0.00326031\n",
      " 0.00320145 0.00320096 0.00311225 0.00311543 0.00309848 0.0030782\n",
      " 0.00304043 0.00301814 0.00300273 0.002998   0.00298492 0.00293321\n",
      " 0.00288272 0.00284673 0.00281811 0.00280483 0.00277779 0.00274119\n",
      " 0.00273441 0.00272243 0.00271764 0.00268047 0.00266405 0.00263714\n",
      " 0.00261629 0.00260627 0.00258516 0.00256405 0.00255967 0.00253091\n",
      " 0.00252215 0.00249661 0.00245966 0.00245895 0.00243202 0.00242694\n",
      " 0.00240047 0.00238942 0.00237139 0.00236128 0.00233396 0.00231696\n",
      " 0.00231438 0.00226703 0.00227577 0.0022655  0.00225296 0.00224604\n",
      " 0.00223603 0.00222111 0.00219597 0.00217991 0.00217405 0.00215817\n",
      " 0.0021453  0.00213352 0.00212103 0.00209512 0.00208777 0.00208116\n",
      " 0.00207925 0.00205585 0.00203888 0.00203019 0.00201371 0.00200225\n",
      " 0.00198327 0.00196367 0.00195863 0.00194818 0.00194404 0.00193588\n",
      " 0.00192689 0.00191236 0.00191299 0.00190621 0.0018887  0.00187837\n",
      " 0.00186689 0.0018637  0.00185112 0.00184218 0.00183598 0.00182739\n",
      " 0.00182407 0.00180993 0.00180704 0.0017928  0.00178058 0.00176894\n",
      " 0.00176247 0.00175058 0.00174302 0.00173961 0.0017307  0.00171172\n",
      " 0.00170521 0.00170365 0.00169781 0.00169092 0.00168301 0.00166607\n",
      " 0.00166319 0.00165352 0.00164371 0.00163914 0.0016271  0.00162409\n",
      " 0.00161998 0.0016098  0.0016003  0.00159184 0.00158723 0.00157308\n",
      " 0.00156567 0.00155889 0.00155514 0.00154907 0.00154472 0.0015395\n",
      " 0.00152806 0.00152542 0.00152179 0.00151236 0.00150614 0.00150173\n",
      " 0.00149194 0.0014889  0.00148109 0.00147069 0.00146568 0.00146079\n",
      " 0.00145787 0.00145074 0.001447   0.0014364  0.00143246 0.00142892\n",
      " 0.00142469 0.00141871 0.0014027  0.00139548 0.00139234 0.00138989\n",
      " 0.00138297 0.00137386 0.00136846 0.00136276 0.00135843 0.00135116\n",
      " 0.00134634 0.00134293 0.00133332 0.00132311 0.00131946 0.00131136\n",
      " 0.00130348 0.00129598 0.0012897  0.00128794 0.00127903 0.00127509\n",
      " 0.00126968 0.00126408 0.00125361 0.00124894 0.00124413 0.00124095\n",
      " 0.00123609 0.00122683 0.00122347 0.00121916 0.00120996 0.00120238\n",
      " 0.00119888 0.00119411 0.00118363 0.0011798  0.00117622 0.0011695\n",
      " 0.00116486 0.00116128 0.00115671 0.00115241 0.00113974 0.00113563\n",
      " 0.0011286  0.00112639 0.00111892 0.00111722 0.00110608]\n",
      "0.6530034309958068\n",
      "[10.1726691   5.23112765  3.93800364  3.89234909  3.76880007  3.62697105\n",
      "  3.53117133  3.48072294  3.45242218  3.43240965  3.37249708  3.29138651\n",
      "  3.23898683  3.19906999  3.17206831  3.14171483  3.09839679  3.03481725\n",
      "  3.00676545  2.99756441  2.94912324  2.92933343  2.89242571  2.86188156\n",
      "  2.85233514  2.83933251  2.79136408  2.78578855  2.73567488  2.7034186\n",
      "  2.68461737  2.66780994  2.62778048  2.58671942  2.57858831  2.55714285\n",
      "  2.53047864  2.52041572  2.50723473  2.49106256  2.4602258   2.45584865\n",
      "  2.40379554  2.40250247  2.38131444  2.36018474  2.3486998   2.33031242\n",
      "  2.30752772  2.30723911  2.28936053  2.2796464   2.26991189  2.26254395\n",
      "  2.25196399  2.2403295   2.23710506  2.23606764  2.22922102  2.20964795\n",
      "  2.19092719  2.17592803  2.16571369  2.16047664  2.14936768  2.13557768\n",
      "  2.13345443  2.12771508  2.1266566   2.11140103  2.10512961  2.09482938\n",
      "  2.0866808   2.08196024  2.07338685  2.06572965  2.06323001  2.05150919\n",
      "  2.04797065  2.03782525  2.02554253  2.02288845  2.01221291  2.00893102\n",
      "  1.99877818  1.99438745  1.98624733  1.98156334  1.97014209  1.9637871\n",
      "  1.96197964  1.94947885  1.94576379  1.94097681  1.9370414   1.932675\n",
      "  1.92829668  1.92191104  1.91125808  1.90395819  1.90138758  1.89443509\n",
      "  1.89017062  1.88424565  1.87812611  1.86990088  1.86328727  1.86035428\n",
      "  1.85947916  1.85095821  1.84176248  1.83810949  1.83005419  1.8248554\n",
      "  1.81650513  1.80705477  1.80472079  1.799896    1.79798372  1.79451129\n",
      "  1.79123346  1.78391896  1.78360476  1.78094952  1.77220957  1.76774121\n",
      "  1.76227399  1.76074495  1.75825353  1.75028941  1.74756214  1.74337441\n",
      "  1.74251287  1.73485855  1.73348704  1.72730881  1.72085039  1.71538652\n",
      "  1.71262723  1.70801705  1.70287273  1.70082433  1.69654448  1.68734986\n",
      "  1.68411134  1.6831654   1.68065682  1.67685105  1.67293404  1.66462983\n",
      "  1.66305646  1.65821411  1.65346766  1.65115655  1.64533228  1.64339851\n",
      "  1.64137009  1.63628759  1.63139811  1.62704049  1.62462454  1.6177379\n",
      "  1.61370206  1.6103988   1.60815382  1.60583907  1.60371816  1.60028238\n",
      "  1.59414609  1.59282278  1.59149323  1.58584991  1.58287939  1.58051262\n",
      "  1.57510155  1.57352085  1.56962441  1.56390155  1.56119217  1.55858745\n",
      "  1.55755446  1.55363571  1.55137447  1.5455227   1.54357948  1.54147955\n",
      "  1.53954498  1.53605073  1.52824281  1.52343816  1.52161597  1.52033014\n",
      "  1.51662327  1.51171351  1.5085508   1.50536812  1.50305831  1.49895532\n",
      "  1.49627743  1.49437326  1.49006104  1.48369463  1.48126518  1.47674063\n",
      "  1.47309758  1.46804315  1.46496776  1.46348763  1.45846682  1.45614295\n",
      "  1.45308353  1.45002249  1.44419895  1.44188566  1.43835514  1.43652603\n",
      "  1.43370205  1.42841442  1.42637333  1.42480173  1.41873547  1.41417834\n",
      "  1.41214767  1.40917875  1.40317778  1.40167355  1.39862329  1.39476497\n",
      "  1.39177922  1.38966212  1.38694376  1.38443003  1.37679988  1.37422556\n",
      "  1.3700854   1.36860489  1.36515228  1.36306423  1.35627067]\n",
      "(239, 1197)\n"
     ]
    }
   ],
   "source": [
    "#lsa를 쓰면?\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=239, n_iter=7, random_state=42) #압축할 component의 수 지정\n",
    "svd.fit(X_train_tfidf)  \n",
    "print(svd.explained_variance_ratio_)  #계산된 각 component가 설명하는 분산의 비율\n",
    "print(svd.explained_variance_ratio_.sum())  #선택된 component들이 설명하는 분산의 합 -> 선택한 component의 수에 따라 달라짐\n",
    "print(svd.singular_values_) \n",
    "print(svd.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\YeonJI\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set score: 0.774\n",
      "Test set score: 0.621\n"
     ]
    }
   ],
   "source": [
    "X_train_svd = svd.transform(X_train_tfidf) #선택된 component를 이용하여 2,000개의 feature로부터 feature extract (dimension reduce)\n",
    "X_test_svd = svd.transform(X_test_tfidf)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "SVD_clf = LogisticRegression()\n",
    "SVD_clf.fit(X_train_svd, y_train)\n",
    "print('Train set score: {:.3f}'.format(SVD_clf.score(X_train_svd, y_train)))\n",
    "print('Test set score: {:.3f}'.format(SVD_clf.score(X_test_svd, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set dimension: (1710, 911)\n",
      "Test set dimension: (570, 911)\n",
      "Train set score: 0.833\n",
      "Test set score: 0.654\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(tokenizer=twit_tokenizer2, min_df=2).fit(X_train) #tfidf와 동일하게 max_feature를 제한하여 학습\n",
    "X_train_cv = cv.transform(X_train) # train set을 변환\n",
    "print('Train set dimension:', X_train_cv.shape) # 36310 대신 2000이 된 것을 확인\n",
    "X_test_cv = cv.transform(X_test) # test set을 변환\n",
    "print('Test set dimension:', X_test_cv.shape)\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "NB_clf = MultinomialNB()\n",
    "NB_clf.fit(X_train_cv, y_train)\n",
    "print('Train set score: {:.3f}'.format(NB_clf.score(X_train_cv, y_train)))\n",
    "print('Test set score: {:.3f}'.format(NB_clf.score(X_test_cv, y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
